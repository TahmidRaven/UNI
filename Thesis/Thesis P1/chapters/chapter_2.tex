% Better predictive modeling depends on better understanding of the data and attributes selection. We have to choose between some data mining algorithm. We have chosen data mining as it is very much flexible in predictive modeling. Prediction when the game is in progress is a tough ask and it need finding the best attributes that influence the match outcome. Some research was done previously on predictive modeling in sports like Basketball, Baseball along with Test and One Day International cricket.
% In basketball, Bhandari et al.\cite{Bhandari1997} developed a knowledge discovery system and data mining framework for National Basketball Association (NBA). It was aimed to discover several interesting patterns in basketball games. This and related system have been used by several basketball teams over the past decades. Such solutions designed for offline usage and no in game effects were taken care of. There has been some recent works (20) about in-game decision making to find how much time remaining in the game without making any prior prediction model.
% There were several works done in cricket. Bailey and Clarke\cite{Bailey} and Sankaranarayanan et al.\cite{Sankaranarayanan} used machine learning approach to predict the result of a one day match depending on the previous data and in game data.
% Akhtar and Scarf\cite{Akhtar} used multinomial logistic regression in their work on predicting a outcome of a test matches played between two teams.
% Choudhury et al.\cite{Choudhury} used Artificial Neural Network to predict result of a multi team one day cricket tournament depending on the past 10 years data. They used training set in order to model the data in neural network. Again there was no in play effects were taken care of.
% For baseball, Ganeshapillai and Guttag\cite{Ganeshapillai} developed a prediction model that decides when to change the starting pitcher as the game progresses. It is very much similar to our work-flow, where they used the combination of previous data and in game data to predict a pitchers performance.
% Tulabandhula and Rudin\cite{Tulabandhula} were designed a real time prediction and decision system for professional car racing. Model makes the decision of when is the best time for tire change and how many of them. These works supplied a huge encouragement and informative ideas in our research.

% Tahmid Iqbal 

Khurshid et al.\cite{doi:10.1161/CIRCULATIONAHA.121.057480}  (2021) explored strategies to improve the very early detection of AF, one of the most prevalent yet generally asymptomatic cardiac conditions that produce significant health risks, including stroke. Current clinical models, such as CHA2DS2-VASc, have limited predictive power. Dealing with this, a model was proposed that implements deep learning on ECG data by using CNNs scanning minute patterns in the signals emanating from ECG, hinting toward future AF risk. The improvement over their model came in terms of creating a hybrid version: CH-AI, which combined ECG-based predictions with risk-associated variables, hence being way more accurate in predictions than classic models like CHARGE-AF. Further, this study was the first to make use of enormous ECG datasets ranging from three different sources such as the UK Biobank and the Brigham and Women's Hospital databases. Additionally, the authors preprocessed and labeled ECG data into both training and validation sets. Similarly, a type of deep learning model that involves CNNs was used by the authors in this experiment, suitable for both time-series data and waveform data e.g. ECG analysis. It has also been trained to identify patterns, even when there is a normal sinus rhythm on the ECG, which might indicate a high risk for AF. Moreover, the researchers have proposed a hybrid model, CH-AI, that integrates clinical risk factors into the deep learning ECG-based models, ECG-AI, in an attempt to improve prediction accuracy. Because of this method, the accuracy was higher compared to clinical or ECG data models. Moreover, saliency maps enhanced the model's decision-making procedure by highlighting features considered important in ECGs. In this regard, the research study established that the model using deep learning outdid the traditional clinical methods in improving the capability of risk prediction for AF, as well as opening a wide range to possible clinical implementation. However, limitations in generalization across populations and studies that rely on retrospective data are only two of a multitude of issues that will make further research necessary in the years to come.

\vspace{0.5cm}


The authors, Ullah et al.\cite{s21030951} (2021), proposed a process that incorporates the cardiac ECG data for classification in enhancing arrhythmias. In this research work, the authors set out to overcome the disadvantages of previous ECG signal analysis methods, which most often failed either in precision or accuracy. Similarly, their very process involves splitting 1D ECG signals with respect to Q-wave peak timings to produce 2D grayscale images (512x512). With this change, classification can now be done using a 2D Convolutional neural network, abbreviated as CNN. The progression is from augmentation of data to increase variety in training data to a CNN architecture that includes many convolutional layers with the ReLU activation, several pooling layers for the extraction of features, as well as the layers that are completely linked or connected fully for classification. We have tuned the model with cross-entropy loss, used an Adam optimizer for back-propagation, and included validation procedures to avoid overfitting. 
 
\vspace{0.5cm}
Moreover, the outcomes from the research show that this strategy greatly enhances accuracy in classification over older approaches. arguably the most significant achievement is the introduction of a scalable, effective, and efficient approach for ECG analysis, resulting in the potential to revolutionize applications in medicine.However, the manuscript also recognizes its limitations, highly depending on high-quality ECG data, and calls for future studies to investigate further with more complex architectures, including additional physiological signals to improve generalizability. The study, in conclusion, emphasizes the potential good that deep learning may offer in improving cardiac healthcare interventions.
 
\vspace{0.5cm}

Galloway et al.\cite{10.1001/jamacardio.2019.0640} (2019) developed a DLM, which they specifically applied to identify hyperkalemia from the analysis of electrocardiogram data obtained from persons with chronic kidney disease. The authors focused on two primary leads because they are effective in identifying signals of height that indicate a level of potassium higher than normal. After that, the model was developed using convolutional neural networks along with recurrent neural networks to efficiently detect both the spatial and temporal patterns present in the ECG data. Thousands of ECGs were collected from a collaboration between the Mayo Clinic and AliveCor; these were matched against serum potassium levels. 
 
\vspace{0.5cm}
The researchers used a five-fold cross-validation approach to improve the efficiency and the precision of the model, applying various signal processing techniques for noise reduction and normalization on the ECG inputs. That means training several variants of DLM with different hyperparameters to enhance the settings. For their evaluation, classic metrics of performance which include AUC, sensitivity, and specificity were considered. The best had an AUC ranging from 0.853 to 0.883, with sensitivities ranging from 88.9\% to 91.3\%. That would mean that this model may find hyperkalemia from ECG data pretty well. The authors also comment on how the false positives can be reduced, elaborating on better ECG-based exclusion criteria for some conditions like left ventricular hypertrophy. Other studies involve further validation of the model for domestic use in reliability at a clinical setting.
 
\vspace{0.5cm}


Vaid et al.\cite{VAID2022395} from their study found that the performance of deep learning models at quantifying biventricular function from electrocardiograms was obtained from the widespread cohort of socioeconomically diverse patients from the Mount Sinai Health System. The authors initiated their study by collecting a large amount of data comprising 700,000-ECGs from over 150,000 unique patients, with the aim of developing models that could predict with high accuracy both left and right ventricular contractile states. They performed a multimodal analysis, where data from ECGs were combined with those from echocardiography, using a sophisticated NLP data analysis pipeline that extracted outcomes from free-text echo reports. Among the results of these tests, their deep learning model showed better performance not only in classifying the LVEF but also in detecting right ventricular dysfunction, with an AUROC as high as 0.84 for recognizing the impaired state of the right ventricle. Later, the authors produced a regression framework to estimate LVEF using an average error of 5.84\%., thus further increasing the clinical value of ECGs as heart failure screening tools. This study demonstrated that their models were truly robust and generalizable, showing that DL techniques could substantially enhance the assessment of cardiac function in patient management within heart failure scenarios. 
 
\vspace{0.5cm}
Liu et al.\cite{pmlr-v85-liu18b} (2018) investigated the possibility of predicting chronic diseases using deep learning algorithms; their work was based on electronic health records and titled "Deep EHR." Their work focused on improving the accuracy and interpretability of the models; much previous work had lacked transparency, which is a vital aspect that clinicians require. Integration of various types, such as numerical and textual information, was underlined as one of the biggest challenges in which high precision had to be maintained. Such authors tested several DL architectures like CNN and LSTM models on the prediction of diseases like congestive heart failure and stroke. The methodology they employed integrated structured data with unstructured medical documentation through the implementation of an innovative negation tagging technique to address the negation present in medical texts, and utilized regular expressions to derive quantitative variables from the data.
 
\vspace{0.5cm}
Their process included both structured data integration, such as numerical values, and unstructured ones like medical notes for model training. They used the CNN models for extracting global features from the data; they tested two different interpretability methods- a gradient-based approach and a log-odds approach-showing greater interpretability for the latter. This study reported strong predictive performance, with overall high AUC scores, which indicated that the CNN model was able to pick out key clinical features such as medical conditions and lifestyle factors. It is because the model performance was so good that issues of precision and computational intensity arose. Further improvements in precision, negation tagging enhancement, and enlargement of the model to be able to predict a greater range of diseases are ways this study can be taken further. This investigation follows and further develops other current efforts in the domain of clinical prediction by using EHRs, where a key balancing challenge between accuracy and interpretability remains open.

 
\vspace{0.5cm}
% Ahtesham Mostafa

In their study, J. Weston Hughes and his colleagues developed a hybrid deep learning model (CH-AI) using Convolutional Neural Networks integrated with traditional clinical risk factors to predict long-term cardiovascular outcomes such as conditions associated with the cardiovascular system including carditis, aorta aneurysms, peripheral vascular disease, embolic diseases, venous thrombotic episodes, congenital heart defects, dysfunction of heart valves, symptoms of heart failure, hypertensive changes in the heart, rheumatic diseases of the heart, myocardial diseases, abnormal rhythm in heart contractions, coronary vascular diseases such as angina and myocardial infarction, and peripheral vascular disorders. When ECG signal analysis and clinical data were combined, the model significantly outperformed previous atrial fibrillation (AF) prediction methods. A notable feature is using saliency maps for interpretability, enhancing clinical trust. Their future research aims to explore real-time applications, wearable device integration, and validation across diverse populations for broader generalizability and continuous monitoring.
\vspace{0.5cm}

Yaqoob Ansari and colleagues in their paper present the overview of DL models for ECG-based detection of arrhythmia and classification developed during the period from 2017 up to 2023. It highlights models such as CNNs, RNNs, and Transformers for their advanced capabilities. The survey offers guidelines for new researchers, focusing on trends, optimizing model architectures, and handling large-scale ECG data. Future improvements include refining DL models for better clinical applications and addressing the challenges of large-scale data processing. The paper serves as a roadmap for those developing DL models in cardiology.
\vspace{0.5cm}


García-Ordás et al.'s research "Heart Disease Risk Prediction Using Deep Learning Techniques with Feature Augmentation" proposes a new multitask neural network model for improved forecasting of heart disease risks. The researchers have used a Sparse Autoencoder (SAE) to augment features and a classifier, including CNNs or Multilayer Perceptrons (MLPs), to perform classification. This approach enhances predictive accuracy by extracting deeper, more meaningful features. Classic methods like decision trees and random forests are also discussed for comparative purposes, with the multitask model demonstrating superior accuracy. The authors suggest refining feature augmentation techniques and expanding model generalization for future research, highlighting its potential for real-world clinical applications.
\vspace{0.5cm}

The study by Gustafsson et al. presents a significant advancement in the diagnosis of myocardial infarction (MI) in emergency department settings by developing and validating a deep learning model that utilizes electrocardiograms (ECGs). Their model, which classifies patients into NSTEMI, STEMI, and control categories, was presented based on a huge research database of 492226 ECGs from 214250 patients within the Stockholm region, drawn between the years 2007 and 2016. It should be noted that the primary algorithm incorporated also worked on patients’ predictors e.g. age and gender with the ECG data. A combination of five models achieved good accuracy, with C-statistics reaching 0.832 for NSTEMI and 0.991 for STEMI, emphasizing the potential of the model as a clinical decision support system. The authors achieve what several previous studies conducted in more controlled environments have not dared to do by stressing the importance of real-world datasets. Future work will focus on improving model generalizability across multiple countries and embedding model usage as a standard practice in the clinics, which, in turn, presents a significant improvement on its own towards early diagnosis and treatment of MI and bettering patient care in emergencies.
\vspace{0.5cm}

The work done by Houssein, Mohamed, and Ali leverages advanced NLP techniques to study the automatic detection of heart disease risk-variables from clinical writings. It also addresses the limitations of previous research that relied on labor-intensive hybrid systems while struggling to extract all relevant risk attributes. In the traditionally complex architectures and instead of the often utilized BERT, CHARACTER-BERT Embedding a sub-character model is used in the proposed framework. This methodology enhances the model’s ability to associate relevant tags and attributes of heart disease risk factors. The i2b2 challenge dataset serves as the primary dataset to demonstrate the effectiveness of their approach, which achieves a remarkable F1 score of 93.66\%, surpassing previous systems used in the challenge. Future work is clearly defined in that, the authors will enhance these detection techniques and also investigate other areas where their methods could be used in the clinics, pointing out that the risk factors and the amount of care given to patients in the health care systems could be improved.
\vspace{0.5cm}


% Mehedi Hasan

This study, entitled "Natural Language Processing to Improve Prediction of Incident Atrial Fibrillation Using Electronic Health Records"\cite{doi:10.1161/JAHA.122.026014} , investigates the application of NLP in improving the prediction of AF risk in older adults through narrative text analyses from EHRs. They developed two predictive models: one based solely on codified EHR data and another that combined codified data with information extracted using NLP. Results across more than 86,000 patients showed that the NLP-enhanced model demonstrated superior predictive performance compared with both the codified-only model and traditional CHARGE-AF score. Both models exhibited good calibration in internal validation but overestimated risk in external validation cohorts. Results therefore indicate that NLP would better identify high-risk patients, which would allow for targeted AF screening and prevention strategies. However, this needs further validation in various settings.
\vspace{0.5cm}

The study entitled "Cohort design and natural language processing to reduce bias in electronic health records research" addressed bias in EHR-based studies by establishing the Community Care Cohort Project, where over 520,000 patients receive primary care. To reduce the amount of missing data, they employed NLP to recover vital signs from clinical notes that were unstructured, thereby reducing missingness by 31\%. Comparing C3PO to the convenience samples showed better calibration of the risk models for heart disease and atrial fibrillation, highlighting reduced bias. For the first time, the authors introduced the JEDI pipeline for processing diverse EHR data, providing a scalable tool for future research. This work improves both the accuracy and generalizability of healthcare research and may have an impact on public health.
\vspace{0.5cm}

The study  "LLMs-based Few-Shot Disease Predictions using EHR: A Novel Approach Combining Predictive Agent Reasoning and Critical Agent Instruction" has looked into the application of large language models to predict diseases with electronic health records, considering that their traditional methods of supervised learning lacked large labeled data. The research team now proposes a new framework called EHR-CoAgent, which involves two collaborative LLM agents: a Predictor Agent that makes the predictions of the disease along with the reasoning behind those predictions, and a Critic Agent that assesses any predictions which are not correct to provide feedback so that further improvements can be made. Structured EHR data are transformed into natural language narratives to explicitly allow for the capability of LLMs to apply their effective reasoning. It also examines the performances of Zero-Shot and Few-Shot LLMs on datasets such as MIMIC-III and CRADLE using various prompting strategies. Results are promising and show that EHR-CoAgent significantly improves diagnostic performance compared to traditional machine learning models, even in few-shot settings. In general, the results highlight the role of LLMs as useful assistants in clinical decision-making for healthcare, and in particular, in the accurate prediction of diseases with minimal training data.
\vspace{0.5cm}

The study\cite{chen2024largelanguagemodelinformedecg}, "Large Language Model-informed ECG Dual Attention Network for Heart Failure Risk Prediction", a novel ECG-based dual-attention network that predicts heart failure risk in light of the face of rising global HF mortality, capturing intricate cross-lead interactions and local dynamics of 12-lead ECGs for enhanced interpretability and better predictive accuracy. To mitigate limitations in the available data and get a better learning of features, the network is pre-trained using the large language model with ECG reports by aligning ECG features with clinical knowledge. Further, this is fine-tuned on two UK Biobank cohorts of hypertensive patients and those with a history of myocardial infarction. Among these, the proposed approach achieved the best performance, with high C-index scores of 0.6349 for hypertension and 0.5805 for myocardial infarction, outperforming traditional methods. This reflects great potential to further improve early detection and risk stratification of HF. Moreover, this model is further improved in interpretability with a dual attention mechanism, increasing its clinical value.
\vspace{0.5cm}

The study\cite{yu2024ecgsemanticintegratoresi}, "ECG Semantic Integrator (ESI): A Foundation ECG Model Pre Trained with LLM-Enhanced Cardiological Text", proposes the ECG Semantic Integrator, namely ESI, a deep learning-based multimodal framework, to further advance the analysis of ECGs. ESI enhances the semantic comprehension of 12-lead ECG signals through two means:. CQA relies on LLMs and the knowledge of medicine which can generate enriched textual-descriptions of ECG, including demographic and waveform information. The ECG Semantic Integrator then couples the ECG signals with these textual descriptions via contrastive and captioning losses, pretraining the ECG encoders and giving rise to more robust signal representations. It has been tested on large datasets and shows significant improvement over the existing methods of supervised and self-supervised fashion on tasks such as arrhythmia detection and patient identification, thus showing the potential of multimodal learning combined in enhancing cardiac diagnostics.
\vspace{0.5cm}


% Imranul Hasan Emon


Insufficient diagnostic coding in claims data is the foundation of our current methods for identifying hospitalisations for worsening heart failure (WHF). In their work, Ambrosy et al.\cite{10.1001/jamanetworkopen.2021.35152} (2021) discuss the limitations of our current approach and demonstrate how underreporting leads to a mischaracterization of the actual burden of WHF. The study intends to increase the precision of WHF detection by using the NLP algorithm for both structured and unstructured electronic health records (EHRs). The authors employed natural language processing (NLP) methods to analyze pertinent free text notes found in the electronic health record (EHR) within the 72 hours preceding an admission. A regular expression technique was used to separate all the notes to detect line endings and specified section headings.  Then  I2E software(version 6.2.0) was used to extract relevant information by applying custom ontologies to capture accurate clinical data.  Two doctors reviewed and validated the NLP algorithms manually in order to verify them to a criterion standard. The model was tested against manual record reviews and it shows excellent sensitivity (98\%) and specificity (95\%). Compared to our current existing approaches that solely rely on principal discharge diagnoses, the detection of WHF hospitalisations is doubled when the NLP algorithm is applied.  During the study period, hospitalisations for WHF increased significantly, especially among those who have HFpEF. These results indicate that WHF is a far more common issue than we previously thought.
\vspace{0.5cm}

Combining rule-based natural language processing (NLP) with both unstructured and structured data offers a more thorough and precise approach to WHF identification. In contrast to earlier studies which mostly relied on claims data, this method makes use of real-time EHR data to increase sensitivity without compromising specificity. This study not only finds more WHF cases but also reveals clinical subgroups and temporal trends, particularly in underreported populations like those who have HFpEF. The study has some limitations. Because of inconsistent documentation, it does not include long-term therapies such as dialysis and IV vasoactive medicines. In order to distinguish acute WHF from issues that develop later in hospitalisation, the study also concentrated on data from the first 72 hours of admission. Another disadvantage is the potential for hospitalisations that began at non-affiliated facilities may go unreported. Future studies should apply NLP to ambulatory care settings in order to reduce reliance on hospital-based data alone and better capture the patient's journey throughout several healthcare environments.
\vspace{0.5cm}

In a research, Nagamine et al. (2022) divided HF patients into 23 different disease states using unsupervised clustering by K-means based on complaints taken from unstructured EHRs. Patient snapshots were vectorised using TF-IDF, which aids in identifying and classifying clusters and comparable disease states. Using a big dataset, this study examines the complex nature of HF. The authors show diverse HF illness states by clustering patients into temporal snapshots of their data. Instead of structured data, the analysis of unstructured clinical notes through natural language processing (NLP) improves the dataset with clinical complaints, symptoms and patient outcomes. This provides greater insights and information into the course of heart failure. The focus of this study is to enhance our understanding and knowledge of heart failure as a disease by identifying particular disease states and evolution paths which are often ignored in our traditional classification methods.

\vspace{0.5cm}

Pachiyannan et al.\cite{technologies12010004} (2024) propose an automated detection methodology to improve the accuracy of congenital heart disease (CHD) identification. The authors have proposed the ML-CHDPM machine learning model for the early diagnosis of congenital heart disease which uses ECG signals as input. In their model, they used attention Mechanisms (AMs), Convolutional Neural Networks (CNN), and Bidirectional Long Short-Term Memory (BiLSTM) networks. BiLSTM was used to track the temporal dependencies in time-series data while CNNs serve as feature extractors for the ECG signals. Attention mechanisms enhances predictions further paying emphasis on the important aspects of time series. This approach was applied on the database obtained from the ECGs of the pregnant women. The model proposed in this paper attained an accuracy rate of 96.51\% which is higher than just CNN, Random Forest (RF) and Naïve Bayes (NB) methods. Furthermore, the accuracy, recall, and specificity measures performed better than other methods which are already in use with an average precision of 89.14\% and recall of 99.19\%. The use of complex neural network architectures like CNN, BiLSTM, and Attention Mechanisms to a particular dataset consisting of ECGs of pregnant women makes their article notable. This method not only fills a research gap on cardiovascular problems during pregnancy but also improves the diagnosis accuracy of CHD. The primary limitation of this study is that it focuses only on ECG signals. Other data such as genetic or lifestyle information could enhance the predictions even further. In future work, researchers can work on a more diverse and larger dataset. Additionally, this model would be more practical for real-time healthcare situations by improving computing efficiency and shortening training time.
\vspace{0.5cm}


In a study by Sattar et al.\cite{s24082484} (2024), a digital dataset made from ECG images collected from multiple medical facilities is used where the authors used some deep learning models like Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTM) networks and Self-Supervised Learning (SSL) models to their collected data. CNN is the primary model in their study because of its capacity to recognise spatial and temporal patterns in ECG data. An open-source software is employed that transforms the ECG images into time-series signals in order to fit ECG images into their deep learning model. The CNN is intended to analyze features of the ECG signals and classify them into the four groups. Those four groups are normal, myocardial infarction (MI), abnormal heartbeat (AHB) and prior MI (PMI). Each of the groups contains unique features. The classification accuracy achieved by the presented CNN model nearly 92\% . This high accuracy exceeded all the other models tested including LSTM and SSL. High accuracy reported by the CNN model indicates the possibility of using it for the detection of arrhythmias from ECG signals. This model’s effectiveness can also be attributed to a fast inference time of 0.1 seconds per prediction and making it possible for real time clinical monitoring. This study demonstrates the ability of CNN to analyze complicated ECG signals and produce accurate results. But there are some limitations also. The authors uses a very small size of dataset (928 samples) in its execution. This small dataset may also affect the flexibility of the model although the authors employed SMOTE to alleviate this by balancing the class distributions also.  Future studies to address this issue may concentrate on either obtaining more ECG data or using data augmentation methods.  Also, a diverse dataset should be the focus to enhance the scope of the model in subsequent works.
\vspace{0.5cm}


A study by Daydulo et al. (2023) reveals that doctors can minimize mistakes and the time taken to diagnose if they employ automated deep-learning algorithms. These algorithms are able to quickly detect and categorize any cardiac arrhythmia. This method uses time-frequency ECG data to classify three conditions namely congestive heart failure (CHF), cardiac arrhythmia (ARR) and normal sinus rhythm (NSR). The main goal of this study was to create an automated and deep-learning-based ECG segmentation system that can precisely classify ECG data into these three conditions. Their approach comprises of preprocessing the information, converting ECG signals into vision time-frequency representations and classifying the information by the use of deep learning models trained beforehand. Their database was collected from PhysioNet particularly the MIT-BIH database and the BIDMC database. 1D ECG time-series data was enhanced into 2D images using the Morse wavelet. Three categories were created by refining the pre-trained AlexNet and ResNet50 models to classify the ECG data. The accuracy and resilience scores of ResNet50 were higher than those of AlexNet. Hyperparameters including a batch size of 30 and a learning rate of 0.0001 were used in the training process along with batch normalisation and the Adam optimiser. The proposed deep learning model in this study performs exceptionally well in categorisation. Because the model's overall accuracy was 99.2\%. with an average F1-score of 99.\%, sensitivity of 99.2\%. and specificity of 99.6\%. The authors achieve better results than previous models like AlexNet. This work shows that using deep learning models such as ResNet50 in conjunction with Morse wavelet-based time-frequency representations may detect cardiac arrhythmias from ECG data with remarkable efficiency and accuracy.

